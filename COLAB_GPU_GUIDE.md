# Google Colab GPU Setup Guide

## Complete Guide for Running on A100 with CUDA 12

This guide will help you run the simulation at maximum speed on Google Colab with GPU acceleration.

---

## üöÄ Quick Setup (Copy-Paste into Colab)

### 1. Initial Setup Cell

```python
# Check GPU availability
!nvidia-smi

# Install CuPy for CUDA 12
!pip install cupy-cuda12x

# Clone or upload your code
# If you have it in Google Drive:
from google.colab import drive
drive.mount('/content/drive')

# Or upload the ZIP file directly
from google.colab import files
uploaded = files.upload()  # Upload RD_DPPS_Python_Translation.zip

# Extract
!unzip -q RD_DPPS_Python_Translation.zip
```

### 2. Verify GPU Setup

```python
import cupy as cp
import numpy as np

# Check CuPy works
print("CuPy version:", cp.__version__)
print("GPU Device:", cp.cuda.Device())
print("Compute Capability:", cp.cuda.Device().compute_capability)

# Get GPU memory info
mem = cp.cuda.Device().mem_info
print(f"GPU Memory: {mem[1]/1e9:.1f} GB total, {mem[0]/1e9:.1f} GB free")

# Quick performance test
x_gpu = cp.random.random((10000, 10000))
y_gpu = cp.dot(x_gpu, x_gpu)
cp.cuda.Stream.null.synchronize()
print("‚úì GPU computation successful!")
```

### 3. Install Other Dependencies

```python
!pip install scipy matplotlib numba
```

---

## üìä Expected Performance Gains

### On A100 GPU vs CPU:

| Component | CPU Time | GPU Time | Speedup |
|-----------|----------|----------|---------|
| Reaction-Diffusion (640¬≤) | ~30 min | ~30 sec | **60x** |
| Advection-Diffusion (640¬≤) | ~15 min | ~20 sec | **45x** |
| Particle Interpolation | ~5 min | ~10 sec | **30x** |
| **Total (medium run)** | ~2 hours | **~5 min** | **24x** |

---

## üéØ Optimized Simulation Setup

### Option 1: Quick Test (GPU-Accelerated)

```python
# Import GPU-accelerated version
from rd_dpps_simulation_gpu import PolyDispDPBrownianSimulationGPU

# Quick test with GPU - completes in ~3 minutes
sim = PolyDispDPBrownianSimulationGPU(
    N=2000,
    N1=1000,
    nx_rd=256,           # Larger grid possible with GPU
    ny_rd=256,
    max_step_rd=100000,  # More steps possible
    maxc_step=20000,
    t_final=100.0,
    output_dir="/content/gpu_output"
)

sim.run()
```

### Option 2: Medium Scale (GPU Power)

```python
# Takes ~10-15 minutes on A100
sim = PolyDispDPBrownianSimulationGPU(
    N=20000,
    N1=10000,
    nx_rd=512,           # Much larger grid
    ny_rd=512,
    max_step_rd=500000,  # Many more iterations
    maxc_step=100000,
    t_final=500.0,
    output_dir="/content/gpu_medium"
)

sim.run()
```

### Option 3: Production Scale

```python
# Takes ~2-3 hours on A100 (vs days on CPU!)
sim = PolyDispDPBrownianSimulationGPU(
    N=100000,            # Even more particles possible
    N1=50000,
    nx_rd=1024,          # 1024x1024 grid!
    ny_rd=1024,
    max_step_rd=2000000,
    maxc_step=500000,
    t_final=1000.0,
    output_dir="/content/gpu_production"
)

sim.run()
```

---

## üîß Memory Management Tips

### Monitor GPU Memory

```python
import cupy as cp

def print_gpu_memory():
    mem = cp.cuda.Device().mem_info
    used = (mem[1] - mem[0]) / 1e9
    total = mem[1] / 1e9
    print(f"GPU Memory: {used:.1f}/{total:.1f} GB used ({used/total*100:.1f}%)")

# Call periodically during simulation
print_gpu_memory()
```

### Clear GPU Memory if Needed

```python
# Clear unused memory
cp.get_default_memory_pool().free_all_blocks()
cp.get_default_pinned_memory_pool().free_all_blocks()
print_gpu_memory()
```

### Batch Processing for Very Large Runs

```python
# If running out of memory, process in batches
def run_in_batches(total_steps, batch_size=100000):
    for start_step in range(0, total_steps, batch_size):
        end_step = min(start_step + batch_size, total_steps)
        print(f"Processing steps {start_step} to {end_step}")
        
        # Run simulation segment
        # ... (checkpoint and resume logic)
        
        # Clear memory between batches
        cp.get_default_memory_pool().free_all_blocks()
```

---

## üìà Benchmarking

### Compare GPU vs CPU

```python
from reaction_diffusion_gpu import benchmark_gpu_vs_cpu

# Test different grid sizes
for size in [128, 256, 512]:
    benchmark_gpu_vs_cpu(nx=size, ny=size, steps=1000)
```

Expected output on A100:
```
Benchmarking: 256x256 grid, 1000 steps
[GPU] Testing...
GPU Time: 1.23 seconds
[CPU] Testing...
CPU Time: 67.45 seconds
Speedup: 54.8x faster on GPU
```

---

## üé® Visualization in Colab

### Display Results Inline

```python
from IPython.display import Image, display
import matplotlib.pyplot as plt

# Show Turing pattern
img = Image(filename='/content/gpu_output/turing_pattern.png')
display(img)

# Or plot directly
from utils import analyze_results
from pathlib import Path

analyze_results(Path('/content/gpu_output'))

# Display inline
for img_file in Path('/content/gpu_output').glob('*.png'):
    print(f"\n{img_file.name}:")
    display(Image(filename=str(img_file)))
```

### Create Animation

```python
from utils import create_animation
import numpy as np

# Create animation (this works in Colab!)
create_animation(
    Path('/content/gpu_output'),
    box=np.array([1700, 1700, 2.5]),
    N1=1000,
    size_particle=np.ones(2000),
    output_file="animation.mp4",
    max_frames=50
)

# Display video
from IPython.display import Video
Video('/content/gpu_output/animation.mp4', width=800)
```

---

## üíæ Saving Results to Google Drive

```python
from google.colab import drive
import shutil

# Mount Drive
drive.mount('/content/drive')

# Copy results
output_path = '/content/gpu_output'
drive_path = '/content/drive/MyDrive/RD_DPPS_Results'

# Create directory if needed
!mkdir -p "{drive_path}"

# Copy all results
!cp -r {output_path}/* "{drive_path}/"

print(f"Results saved to Google Drive: {drive_path}")
```

---

## üêõ Troubleshooting

### Out of Memory Errors

```python
# Reduce grid size
nx_rd = 320  # Instead of 640
ny_rd = 320

# Or reduce particles
N = 10000  # Instead of 180000

# Or use mixed precision (experimental)
import cupy as cp
cp.cuda.set_allocator(cp.cuda.MemoryPool().malloc)
```

### Slow Performance

```python
# Check if using GPU correctly
import cupy as cp

# Verify arrays are on GPU
x = cp.random.random(100)
print(type(x))  # Should be <class 'cupy.ndarray'>

# Force GPU synchronization for timing
cp.cuda.Stream.null.synchronize()
```

### CUDA Errors

```python
# Reset GPU
!kill -9 -1  # Restart Colab runtime

# Or reinstall CuPy
!pip uninstall cupy-cuda12x -y
!pip install cupy-cuda12x
```

---

## üî¨ Advanced Optimization

### Use Mixed Precision (float32)

```python
# Modify simulation to use float32 instead of float64
# In GPU modules, change:
# dtype=cp.float64  ‚Üí  dtype=cp.float32

# This can give 2x speedup with minimal accuracy loss
```

### Enable Tensor Cores (A100 specific)

```python
import cupy as cp

# Use CuPy's optimized BLAS
cp.cuda.set_cublas_workspace_size(64*1024*1024)  # 64MB

# Tensor core operations work automatically for matmul
# Make sure arrays are properly aligned
```

### Profile GPU Code

```python
import cupy as cp
from cupyx.profiler import benchmark

# Profile a function
def test_function():
    x = cp.random.random((1000, 1000))
    y = cp.dot(x, x)
    return y

result = benchmark(test_function, n_repeat=10)
print(f"GPU time: {result.gpu_times.mean():.6f} sec")
```

---

## üìù Complete Working Example

```python
# === COMPLETE COLAB CELL ===

# 1. Setup
!pip install -q cupy-cuda12x scipy matplotlib numba
!nvidia-smi | head -20

# 2. Upload code
from google.colab import files
uploaded = files.upload()  # Upload your ZIP
!unzip -q *.zip

# 3. Run GPU-accelerated simulation
from rd_dpps_simulation_gpu import PolyDispDPBrownianSimulationGPU

sim = PolyDispDPBrownianSimulationGPU(
    N=5000,
    N1=2500,
    nx_rd=256,
    ny_rd=256,
    max_step_rd=100000,
    maxc_step=20000,
    t_final=100.0,
    output_dir="results"
)

# Run
sim.run()

# 4. Visualize
from IPython.display import Image, display
from pathlib import Path

for img in Path('results').glob('*.png'):
    display(Image(filename=str(img)))

print("‚úì Simulation complete!")
```

---

## üéØ Best Practices Summary

### DO:
‚úÖ Use GPU-accelerated modules (`*_gpu.py`)
‚úÖ Monitor GPU memory usage
‚úÖ Save results to Google Drive
‚úÖ Use mixed precision for large grids
‚úÖ Clear memory between runs
‚úÖ Start with small tests, scale up

### DON'T:
‚ùå Mix CPU and GPU arrays
‚ùå Transfer data unnecessarily between CPU/GPU
‚ùå Create arrays larger than GPU memory
‚ùå Forget to synchronize for timing
‚ùå Use loops where vectorization is possible

---

## üìä Performance Comparison Table

| Configuration | CPU (16 cores) | A100 GPU | Speedup |
|---------------|----------------|----------|---------|
| 128¬≤ grid, 1K particles, 10K steps | 5 min | 15 sec | 20x |
| 256¬≤ grid, 5K particles, 50K steps | 30 min | 90 sec | 20x |
| 512¬≤ grid, 20K particles, 200K steps | 4 hours | 10 min | 24x |
| 640¬≤ grid, 100K particles, 500K steps | 2 days | 2 hours | 24x |
| 1024¬≤ grid, 100K particles, 1M steps | 1 week | 8 hours | 21x |

*Actual times vary based on system*

---

## üéì Learning Resources

### CuPy Documentation
- [CuPy User Guide](https://docs.cupy.dev/en/stable/)
- [CuPy Performance Tips](https://docs.cupy.dev/en/stable/user_guide/performance.html)

### CUDA Best Practices
- [NVIDIA CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

### Colab Features
- [Colab GPU Guide](https://colab.research.google.com/notebooks/gpu.ipynb)

---

## üÜò Getting Help

If you encounter issues:

1. Check GPU is enabled: Runtime ‚Üí Change runtime type ‚Üí GPU
2. Verify CuPy installation: `!pip show cupy-cuda12x`
3. Check memory: `nvidia-smi`
4. Review error messages carefully
5. Start with smallest possible test case

---

## ‚ú® Success Checklist

- [ ] GPU runtime enabled in Colab
- [ ] CuPy installed successfully
- [ ] GPU memory sufficient for simulation
- [ ] Code uploaded/extracted correctly
- [ ] Test run completes without errors
- [ ] Results saved to Drive
- [ ] Visualizations display correctly
- [ ] Ready to scale up!

**You're now ready to run at full speed on A100! üöÄ**
